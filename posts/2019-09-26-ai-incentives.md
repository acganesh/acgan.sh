---
author:
- '[Adithya C. Ganesh]{.smallcaps}'
bibliography:
- 'refs.bib'
nocite: '[@*]'
title: Incentives for AI practitioners are broken
---

(CSS in progress for margin notes).

# Incentives for AI practitioners are broken

I came into Stanford University eager to study artificial intelligence, enthusiastic about building the future. After two years of career fairs, startup pitches, and fireside chats, I grew frustrated by blatant, excessive commercialism<label class="margin-toggle sidenote-number"></label><span class="sidenote">Reads like satire, but: http://nymag.com/intelligencer/2019/09/how-to-network-through-stanford-university.html</span> on display across campus culture. While I did find a small, motivated cohort of students fascinated by solving fundamental problems in AI, they were often drowned out by the industry-oriented majority. Yes, corporations are remarkably prolific when it comes to [research](https://www.loc.gov/rr/scitech/trs/trsosrd.html) [published](https://www.loc.gov/rr/scitech/trs/trsosrd.html) in major conferences.  But, to truly benefit society, academic research must pursue ideas for the long-term future. I am skeptical that the incentive structure for AI research is currently aligned with this goal.

I believe our field misunderstands the categorical difference between “basic research” and “applied research.”  Office of Scientific Research and Development ([OSRD](https://www.loc.gov/rr/scitech/trs/trsosrd.html)) director Vannevar Bush asserted that basic research “results in general knowledge and an understanding of nature by its laws” in his 1945 [report to Roosevelt](https://www.nsf.gov/od/lpa/nsf50/vbush1945.htm).  In the context of AI, this means we ought to think harder about foundational questions like [“What is intelligence?”](https://www.osti.gov/biblio/81587) or [“How do babies learn?”](https://www.jstor.org/stable/26002102?casa_token=gvzR-qdEHocAAAAA:CpQuafzkBUrhFazk_-jOn8hFoHOBIsOs4k5s_c59FH34Aq58T4u-XXzyWXCjPr9VanBNIN255bDz1bEf4_ZJK3cpm3JLa58jNJF751DUoIl28Wkl7zQ#metadata_info_tab_contents) Excessive [industry funding](https://www.nytimes.com/2019/09/06/technology/when-the-ai-professor-leaves-students-suffer-study-says.html) in our field leaves it hyper-focused on engineering, at the expense of this kind of long-term exploration.

On the other hand, applied research refers to taking a broad theoretical idea (say, [convolutional neural networks](http://cs231n.stanford.edu/)), and transferring it to a particular problem of interest (like [facial recognition](https://arxiv.org/abs/1902.03524)). To be clear, applied research is necessary (Michael: I’d change to important); this is why OpenAI took a [$1B investment](https://openai.com/blog/microsoft/) from Microsoft to leverage cloud computing to scale neural network training pipelines.  But it is no substitute for basic research like the [psychology](https://psycnet.apa.org/record/1998-07463-000) of face perception, which has established the field of [visual neuroscience](https://www.cambridge.org/core/journals/visual-neuroscience).

In 1948, Claude Shannon published [“A Mathematical Theory of Communication”](http://www.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) in the Bell System Technical Journal.  Broadly, the paper uses probability theory to analyze [compression](http://isl.stanford.edu/~abbas/ee376b/lect02.pdf) and [communication](https://web.stanford.edu/~dntse/Chapters_PDF/Fundamentals_Wireless_Communication_chapter5.pdf).  While Shannon’s paper focuses on proving theorems, in retrospect it laid the foundations for [encryption](https://www.scientificamerican.com/article/claude-e-shannon-founder/), [MP3 files](http://www.mee.tcd.ie/~corrigad/4c8/A_Brief_Introduction_to_Information_Theory_and_Coding.pdf), and the [Internet](https://www.theguardian.com/science/2014/jun/22/shannon-information-theory).  [Bell Labs](https://www.bell-labs.com/about/history-bell-labs/), where Shannon and his [eminent colleagues](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html) worked, found the right balance between basic and applied research, and we now have the transistor, the laser, and C++.

Given this misalignment of incentives, what should we do?  First, like the editorial boards of [medical journals](https://www.nejm.org/doi/full/10.1056/NEJMc1212744), we must scrutinize funding sources.  While it may be a net positive that industry supports a great deal of [Ph.D research](https://www.cs.cmu.edu/~gradfellowships/), it is alarming that many researchers are [funded by firms](https://www.vice.com/en_us/article/3dxkej/ubers-ai-hub-in-pittsburgh-gutted-a-university-lab-now-its-in-toronto) that have [questionable](https://www.nytimes.com/2019/03/13/technology/facebook-data-deals-investigation.html) [track](https://www.nytimes.com/2019/05/15/business/facial-recognition-software-controversy.html) [records](https://www.nytimes.com/2017/06/06/technology/uber-fired.html).  In January 2017, as a college sophomore, I signed the paperwork for an internship at [Uber ATG](https://www.uber.com/us/en/atg/) to work on self-driving cars; after reading headlines, I felt troubled by the prospect of working at a company with an [aggressive culture](https://www.nytimes.com/2017/02/22/technology/uber-workplace-culture.html), so I chose to renege on the job offer.  In a twist of poignant irony, [Anthony Levandowski’s](https://www.nytimes.com/2017/02/22/technology/uber-workplace-culture.html) signature was printed on my non-disclosure agreement.

Second, we need to have better [meta-analysis](https://www.metascience2019.org/) tools for published research; specifically, software that allows people to understand who’s working on what. These tools have the potential to address many of the [frustrations](https://arxiv.org/pdf/1807.03341.pdf) [practitioners](https://www.alexirpan.com/2018/02/14/rl-hard.html) [express](https://arxiv.org/abs/1709.06560) about AI scholarship.

Third, academia must keep pace. While we may hope that visionaries are drawn to unsolved problems for their own sake, too many of our brightest minds will be drawn to finding slightly [more efficient ways to sell ads](https://www.nytimes.com/2015/03/08/technology/on-the-case-at-mount-sinai-its-dr-data.html) as long as they can get paid [ten—or, in some cases, a hundred—times more](https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html) for doing so.  Academia must “fight fire with fire” and provide an order of magnitude [more funding and tenure track positions](https://twitter.com/stanfordnlp/status/1170804607238303744) for AI research.  Stanford must leverage its endowment to improve [living standards for PhD students](https://www.stanforddaily.com/2019/02/28/every-day-was-about-survival-inside-the-graduate-student-affordability-crisis/), while pushing for increased federal spending in AI.

In my view, developing [safe AI](https://futureoflife.org/ai-safety-research/) is the most important problem of our time. It is the difference between the [catastrophic failure of AI-propagated filter bubblessocial media ech in 2016](https://www.vox.com/technology/2018/12/21/18149099/delete-facebook-scandals-2018-cambridge-analytica) and applying [AI to medicine](https://www.nytimes.com/2019/03/11/well/live/how-artificial-intelligence-could-transform-medicine.html) to save lives. While the current state is unfortunate, a better future is possible. I am optimistic that with funding disclosure, meta-analysis tools, and radically improved support for graduate students,more basic research funding we can ensure that AI practitioners abide by a [Hippocratic Oath](https://www.wired.com/story/should-data-scientists-adhere-to-a-hippocratic-oath/).

# Acknowledgments

It's very difficult to list everyone here, but I am very grateful for my friends and colleagues at Stanford, The Gradient, The Thiel Foundation, 1517, and Neo, for helpful conversations and feedback on these ideas.  I would especially like to thank Michael Swerdlow (Stanford), Amit Ghorawat (Google), Jeff Hammerbacher (Hammer Lab), and Zhanpei Fang (Stanford) for their detailed feedback.


-- 

